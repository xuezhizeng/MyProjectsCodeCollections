{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 1.17 s, total: 14.3 s\n",
      "Wall time: 29.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "\n",
    "# from keras import backend as K\n",
    "# K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=24, inter_op_parallelism_threads=24)))\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import IPython.display as ipd\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "from time import time, ctime\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "from time import time, ctime\n",
    "\n",
    "L = 16000\n",
    "legal_labels = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "\n",
    "#src folders\n",
    "root_path = r'.'\n",
    "out_path = r'.'\n",
    "model_path = r'.'\n",
    "train_data_path = os.path.join(root_path, 'data', 'train', 'audio')\n",
    "test_data_path = os.path.join(root_path, 'data', 'test', 'audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses.binary_crossentropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_audio(samples):\n",
    "    if len(samples) >= L: return samples\n",
    "    else: return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0, 0))\n",
    "\n",
    "def chop_audio(samples, L=16000, num=20):\n",
    "    for i in range(num):\n",
    "        beg = np.random.randint(0, len(samples) - L)\n",
    "        yield samples[beg: beg + L]\n",
    "\n",
    "def label_transform(labels):\n",
    "    nlabels = []\n",
    "    for label in labels:\n",
    "        if label == '_background_noise_':\n",
    "            nlabels.append('silence')\n",
    "#         elif label not in legal_labels:\n",
    "#             nlabels.append('unknown')\n",
    "        else:\n",
    "            nlabels.append(label)\n",
    "    return pd.get_dummies(pd.Series(nlabels))\n",
    "\n",
    "def custom_fft(y, fs):\n",
    "    T = 1.0 / fs\n",
    "    N = y.shape[0]\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    # FFT is simmetrical, so we take just the first half\n",
    "    # FFT is also complex, to we take just the real part (abs)\n",
    "    vals = 2.0/N * np.abs(yf[0:N//2])\n",
    "    return xf, vals\n",
    "\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "def list_wavs_fname(dirpath, ext='wav'):\n",
    "    print(dirpath)\n",
    "    fpaths = glob(os.path.join(dirpath, r'*/*' + ext))\n",
    "    pat = r'.+/(\\w+)/\\w+\\.' + ext + '$'\n",
    "    labels = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            labels.append(r.group(1))\n",
    "    pat = r'.+/(\\w+\\.' + ext + ')$'\n",
    "    fnames = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            fnames.append(r.group(1))\n",
    "    return labels, fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train/audio\n",
      "1/64 2/64 3/64 4/64 5/64 6/64 7/64 8/64 9/64 10/64 11/64 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shj16110/anaconda2/envs/3.6.1/lib/python3.6/site-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/64 13/64 14/64 15/64 16/64 17/64 18/64 19/64 20/64 21/64 22/64 23/64 24/64 25/64 26/64 27/64 28/64 29/64 30/64 31/64 32/64 33/64 34/64 35/64 36/64 37/64 38/64 39/64 40/64 41/64 42/64 43/64 44/64 45/64 46/64 47/64 48/64 49/64 50/64 51/64 52/64 53/64 54/64 55/64 56/64 57/64 58/64 59/64 60/64 61/64 62/64 63/64 64/64 (64841, 99, 161, 1) (64841, 31) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "CPU times: user 1min 49s, sys: 7.31 s, total: 1min 56s\n",
      "Wall time: 17min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels, fnames = list_wavs_fname(train_data_path)\n",
    "\n",
    "new_sample_rate = 16000\n",
    "y_train = []\n",
    "x_train = []\n",
    "\n",
    "n=0\n",
    "for label, fname in zip(labels, fnames):\n",
    "    n+=1\n",
    "    if n%1000==0:\n",
    "        print(int(n/1000), end='/{} '.format(int(len(labels)/1000)))\n",
    "    sample_rate, samples = wavfile.read(os.path.join(train_data_path, label, fname))\n",
    "    \n",
    "    samples = pad_audio(samples)\n",
    "    \n",
    "    if len(samples) > 16000:\n",
    "        n_samples = chop_audio(samples)\n",
    "    else: n_samples = [samples]\n",
    "        \n",
    "    for samples in n_samples:\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / sample_rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        \n",
    "        y_train.append(label)\n",
    "        x_train.append(specgram)\n",
    "        \n",
    "x_train = np.array(x_train)\n",
    "x_train = x_train.reshape(tuple(list(x_train.shape) + [1]))\n",
    "y_train = label_transform(y_train)\n",
    "\n",
    "label_index = y_train.columns.values\n",
    "\n",
    "y_train = y_train.values\n",
    "y_train = np.array(y_train)\n",
    "del labels, fnames\n",
    "gc.collect()\n",
    "\n",
    "X = x_train.copy()\n",
    "Y = y_train.copy()\n",
    "print (X.shape, Y.shape, type(X), type(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_index = np.array(list(['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go',\n",
    "       'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on',\n",
    "       'one', 'right', 'seven', 'sheila', 'silence', 'six', 'stop',\n",
    "       'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero'])).astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 877 ms, total: 877 ms\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# np.save('X_31label_16k.npy', X)\n",
    "# np.save('Y_31label_16k.npy', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mid start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 1.39 s, total: 1.39 s\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = np.load('X_31label_16k.npy')\n",
    "Y = np.load('Y_31label_16k.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 99, 161, 1)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 99, 161, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 98, 160, 16)       80        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 97, 159, 16)       1040      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 48, 79, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 48, 79, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 46, 77, 21)        3045      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 44, 75, 21)        3990      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 22, 37, 21)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 22, 37, 21)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 20, 35, 36)        6840      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 11, 36)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 6, 11, 36)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2376)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               304256    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 31)                7967      \n",
      "=================================================================\n",
      "Total params: 361,782\n",
      "Trainable params: 361,012\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (99, 161, 1)\n",
    "nclass = 31\n",
    "inp = Input(shape=input_shape)\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "img_1 = Convolution2D(16, kernel_size=2, activation=activations.relu)(norm_inp)\n",
    "img_1 = Convolution2D(16, kernel_size=2, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(21, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = Convolution2D(21, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(36, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(3, 3))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Flatten()(img_1)\n",
    "\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(img_1))\n",
    "dense_1 = BatchNormalization()(Dense(256, activation=activations.relu)(dense_1))\n",
    "dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "model = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/check-point-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# checkpoint\n",
    "filepath=\"models/F36W-31L-{epoch:02d}-{val_acc:.5f}_.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58356, 99, 161, 1) <class 'numpy.ndarray'>\n",
      "(58356, 31) <class 'numpy.ndarray'>\n",
      "(6485, 99, 161, 1) <class 'numpy.ndarray'>\n",
      "(6485, 31) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = lambda *x: [print(i.shape, type(i)) for i in x]\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.1, random_state=2018)\n",
    "S(x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     31.000000\n",
       "mean     209.193548\n",
       "std       47.781739\n",
       "min       16.000000\n",
       "25%      183.500000\n",
       "50%      225.000000\n",
       "75%      240.500000\n",
       "max      261.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_valid).sum().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58356 samples, validate on 6485 samples\n",
      "Epoch 1/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9226Epoch 00000: val_acc did not improve\n",
      "58356/58356 [==============================] - 131s - loss: 0.2515 - acc: 0.9226 - val_loss: 0.2531 - val_acc: 0.9263\n",
      "Epoch 2/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2473 - acc: 0.9245Epoch 00001: val_acc did not improve\n",
      "58356/58356 [==============================] - 133s - loss: 0.2472 - acc: 0.9245 - val_loss: 0.2471 - val_acc: 0.9269\n",
      "Epoch 3/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9256Epoch 00002: val_acc improved from 0.93416 to 0.93431, saving model to models/F36W-31L-02-0.93431_.hdf5\n",
      "58356/58356 [==============================] - 131s - loss: 0.2425 - acc: 0.9255 - val_loss: 0.2290 - val_acc: 0.9343\n",
      "Epoch 4/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9252Epoch 00003: val_acc did not improve\n",
      "58356/58356 [==============================] - 133s - loss: 0.2440 - acc: 0.9252 - val_loss: 0.2334 - val_acc: 0.9338\n",
      "Epoch 5/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9280Epoch 00004: val_acc did not improve\n",
      "58356/58356 [==============================] - 131s - loss: 0.2365 - acc: 0.9280 - val_loss: 0.2344 - val_acc: 0.9308\n",
      "Epoch 6/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9292Epoch 00005: val_acc improved from 0.93431 to 0.93631, saving model to models/F36W-31L-05-0.93631_.hdf5\n",
      "58356/58356 [==============================] - 132s - loss: 0.2303 - acc: 0.9292 - val_loss: 0.2315 - val_acc: 0.9363\n",
      "Epoch 7/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9304Epoch 00006: val_acc did not improve\n",
      "58356/58356 [==============================] - 131s - loss: 0.2281 - acc: 0.9304 - val_loss: 0.2274 - val_acc: 0.9346\n",
      "Epoch 8/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9310Epoch 00007: val_acc did not improve\n",
      "58356/58356 [==============================] - 131s - loss: 0.2240 - acc: 0.9310 - val_loss: 0.2290 - val_acc: 0.9343\n",
      "Epoch 9/100\n",
      "58336/58356 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9315Epoch 00008: val_acc did not improve\n",
      "58356/58356 [==============================] - 133s - loss: 0.2221 - acc: 0.9315 - val_loss: 0.2336 - val_acc: 0.9323\n",
      "Epoch 10/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9319Epoch 00009: val_acc did not improve\n",
      "58356/58356 [==============================] - 131s - loss: 0.2196 - acc: 0.9319 - val_loss: 0.2283 - val_acc: 0.9345\n",
      "Epoch 11/100\n",
      "58336/58356 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9338Epoch 00010: val_acc did not improve\n",
      "58356/58356 [==============================] - 132s - loss: 0.2150 - acc: 0.9338 - val_loss: 0.2328 - val_acc: 0.9343\n",
      "Epoch 12/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9336Epoch 00011: val_acc did not improve\n",
      "58356/58356 [==============================] - 131s - loss: 0.2182 - acc: 0.9336 - val_loss: 0.2328 - val_acc: 0.9351\n",
      "Epoch 13/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9327Epoch 00012: val_acc improved from 0.93631 to 0.93739, saving model to models/F36W-31L-12-0.93739_.hdf5\n",
      "58356/58356 [==============================] - 132s - loss: 0.2202 - acc: 0.9327 - val_loss: 0.2146 - val_acc: 0.9374\n",
      "Epoch 14/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9347Epoch 00013: val_acc improved from 0.93739 to 0.93863, saving model to models/F36W-31L-13-0.93863_.hdf5\n",
      "58356/58356 [==============================] - 131s - loss: 0.2141 - acc: 0.9347 - val_loss: 0.2198 - val_acc: 0.9386\n",
      "Epoch 15/100\n",
      "58336/58356 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9359Epoch 00014: val_acc did not improve\n",
      "58356/58356 [==============================] - 133s - loss: 0.2045 - acc: 0.9359 - val_loss: 0.2218 - val_acc: 0.9346\n",
      "Epoch 16/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9386Epoch 00015: val_acc did not improve\n",
      "58356/58356 [==============================] - 131s - loss: 0.2013 - acc: 0.9386 - val_loss: 0.2206 - val_acc: 0.9360\n",
      "Epoch 17/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9382Epoch 00016: val_acc did not improve\n",
      "58356/58356 [==============================] - 131s - loss: 0.1975 - acc: 0.9381 - val_loss: 0.2193 - val_acc: 0.9382\n",
      "Epoch 18/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9384Epoch 00017: val_acc improved from 0.93863 to 0.94048, saving model to models/F36W-31L-17-0.94048_.hdf5\n",
      "58356/58356 [==============================] - 133s - loss: 0.2009 - acc: 0.9383 - val_loss: 0.2081 - val_acc: 0.9405\n",
      "Epoch 19/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9399Epoch 00018: val_acc did not improve\n",
      "58356/58356 [==============================] - 131s - loss: 0.1954 - acc: 0.9399 - val_loss: 0.2217 - val_acc: 0.9366\n",
      "Epoch 20/100\n",
      "58336/58356 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9395Epoch 00019: val_acc did not improve\n",
      "58356/58356 [==============================] - 133s - loss: 0.1933 - acc: 0.9395 - val_loss: 0.2309 - val_acc: 0.9362\n",
      "Epoch 21/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9412Epoch 00020: val_acc did not improve\n",
      "58356/58356 [==============================] - 131s - loss: 0.1927 - acc: 0.9412 - val_loss: 0.2276 - val_acc: 0.9362\n",
      "Epoch 22/100\n",
      "58336/58356 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9421Epoch 00021: val_acc did not improve\n",
      "58356/58356 [==============================] - 133s - loss: 0.1901 - acc: 0.9421 - val_loss: 0.2219 - val_acc: 0.9379\n",
      "Epoch 23/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9413Epoch 00022: val_acc did not improve\n",
      "58356/58356 [==============================] - 131s - loss: 0.1866 - acc: 0.9413 - val_loss: 0.2308 - val_acc: 0.9346\n",
      "Epoch 24/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.1814 - acc: 0.9434Epoch 00023: val_acc did not improve\n",
      "58356/58356 [==============================] - 133s - loss: 0.1815 - acc: 0.9434 - val_loss: 0.2162 - val_acc: 0.9346\n",
      "CPU times: user 40min 6s, sys: 9min 17s, total: 49min 23s\n",
      "Wall time: 52min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ba8b73aff98>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, batch_size=16, validation_data=(x_valid, y_valid), \n",
    "          epochs=100, shuffle=True, verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(y_valid).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save(os.path.join(model_path, 'cnn.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F36W-31L-00-0.79630_.hdf5\r\n",
      "F36W-31L-00-0.98669_.hdf5\r\n",
      "F36W-31L-01-0.86507_.hdf5\r\n",
      "F36W-31L-01-0.99168_.hdf5\r\n",
      "F36W-31L-02-0.89067_.hdf5\r\n",
      "F36W-31L-02-0.93431_.hdf5\r\n",
      "F36W-31L-02-0.99251_.hdf5\r\n",
      "F36W-31L-03-0.89345_.hdf5\r\n",
      "F36W-31L-04-0.89653_.hdf5\r\n",
      "F36W-31L-05-0.90763_.hdf5\r\n",
      "F36W-31L-05-0.93631_.hdf5\r\n",
      "F36W-31L-06-0.91781_.hdf5\r\n",
      "F36W-31L-08-0.92290_.hdf5\r\n",
      "F36W-31L-12-0.93739_.hdf5\r\n",
      "F36W-31L-13-0.92907_.hdf5\r\n",
      "F36W-31L-13-0.93863_.hdf5\r\n",
      "F36W-31L-14-0.93169_.hdf5\r\n",
      "F36W-31L-16-0.93416_.hdf5\r\n",
      "F36W-31L-17-0.94048_.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "!ls models|grep 36W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Mname = 'models/F36W-31L-17-0.94048_.hdf5'\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model(Mname)\n",
    "\n",
    "legal_label = np.array(['down', 'go', 'left', 'no', 'off', 'on', 'right',\n",
    "                        'silence', 'stop', 'unknown', 'up', 'yes']).astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_data_generator(batch=16):\n",
    "    fpaths = glob(os.path.join(test_data_path, '*wav'))\n",
    "    i = 0\n",
    "    for path in fpaths:\n",
    "        if i == 0:\n",
    "            imgs = []\n",
    "            fnames = []\n",
    "        i += 1\n",
    "        rate, samples = wavfile.read(path)\n",
    "        samples = pad_audio(samples)\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        imgs.append(specgram)\n",
    "        fnames.append(path.split('\\\\')[-1])\n",
    "        if i == batch:\n",
    "            i = 0\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "            yield fnames, imgs\n",
    "    if i < batch:\n",
    "        imgs = np.array(imgs)\n",
    "        imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "        yield fnames, imgs\n",
    "    raise StopIteration()\n",
    "    \n",
    "new_sample_rate=16000\n",
    "\n",
    "label_index = np.array(list(['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go',\n",
    "       'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on',\n",
    "       'one', 'right', 'seven', 'sheila', 'silence', 'six', 'stop',\n",
    "       'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero'])).astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 13.38 s :640 used: 12.36 s :1280 used: 12.49 s :1920 used: 12.64 s :2560 used: 13.15 s :3200 used: 12.99 s :3840 used: 12.73 s :4480 used: 13.19 s :5120 used: 12.61 s :5760 used: 12.95 s :6400 used: 12.38 s :7040 used: 13.26 s :7680 used: 13.35 s :8320 used: 12.91 s :8960 used: 22.01 s :9600 used: 25.01 s :10240 used: 15.79 s :10880 used: 16.66 s :11520 used: 17.45 s :12160 used: 15.16 s :12800 used: 15.36 s :13440 used: 14.92 s :14080 used: 13.48 s :14720 used: 13.47 s :15360 used: 13.15 s :16000 used: 12.76 s :16640 used: 13.35 s :17280 used: 13.06 s :17920 used: 13.25 s :18560 used: 13.00 s :19200 used: 13.22 s :19840 used: 13.06 s :20480 used: 12.62 s :21120 used: 12.87 s :21760 used: 13.00 s :22400 used: 13.07 s :23040 used: 13.58 s :23680 used: 13.09 s :24320 used: 12.77 s :24960 used: 13.27 s :25600 used: 13.05 s :26240 used: 12.74 s :26880 used: 12.78 s :27520 used: 12.98 s :28160 used: 12.58 s :28800 used: 12.34 s :29440 used: 12.67 s :30080 used: 14.11 s :30720 used: 13.64 s :31360 used: 13.74 s :32000 used: 13.13 s :32640 used: 13.48 s :33280 used: 14.31 s :33920 used: 13.48 s :34560 used: 13.83 s :35200 used: 14.56 s :35840 used: 13.87 s :36480 used: 14.55 s :37120 used: 13.39 s :37760 used: 13.67 s :38400 used: 13.77 s :39040 used: 13.99 s :39680 used: 13.85 s :40320 used: 12.69 s :40960 used: 13.83 s :41600 used: 12.43 s :42240 used: 12.28 s :42880 used: 13.67 s :43520 used: 13.65 s :44160 used: 12.86 s :44800 used: 12.84 s :45440 used: 13.82 s :46080 used: 13.08 s :46720 used: 14.09 s :47360 used: 13.90 s :48000 used: 12.74 s :48640 used: 12.94 s :49280 used: 13.71 s :49920 used: 12.77 s :50560 used: 13.24 s :51200 used: 13.09 s :51840 used: 13.59 s :52480 used: 13.85 s :53120 used: 13.20 s :53760 used: 12.92 s :54400 used: 13.42 s :55040 used: 13.52 s :55680 used: 13.74 s :56320 used: 14.74 s :56960 used: 12.77 s :57600 used: 13.74 s :58240 used: 13.10 s :58880 used: 13.06 s :59520 used: 13.58 s :60160 used: 14.80 s :60800 used: 13.24 s :61440 used: 13.45 s :62080 used: 13.55 s :62720 used: 12.98 s :63360 used: 12.90 s :64000 used: 13.63 s :64640 used: 13.33 s :65280 used: 28.15 s :65920 used: 17.29 s :66560 used: 17.11 s :67200 used: 16.30 s :67840 used: 15.88 s :68480 used: 13.95 s :69120 used: 12.68 s :69760 used: 13.38 s :70400 used: 12.91 s :71040 used: 13.47 s :71680 used: 12.76 s :72320 used: 13.76 s :72960 used: 13.63 s :73600 used: 13.47 s :74240 used: 13.68 s :74880 used: 13.39 s :75520 used: 12.67 s :76160 used: 12.77 s :76800 used: 12.89 s :77440 used: 12.42 s :78080 used: 14.22 s :78720 used: 12.81 s :79360 used: 12.62 s :80000 used: 13.17 s :80640 used: 13.31 s :81280 used: 12.79 s :81920 used: 12.74 s :82560 used: 12.99 s :83200 used: 12.83 s :83840 used: 13.98 s :84480 used: 23.21 s :85120 used: 40.30 s :85760 used: 17.21 s :86400 used: 17.04 s :87040 used: 17.08 s :87680 used: 18.79 s :88320 used: 18.24 s :88960 used: 18.20 s :89600 used: 16.66 s :90240 used: 16.61 s :90880 used: 17.08 s :91520 used: 13.67 s :92160 used: 12.78 s :92800 used: 13.04 s :93440 used: 13.84 s :94080 used: 13.06 s :94720 used: 13.30 s :95360 used: 13.42 s :96000 used: 12.88 s :96640 used: 13.63 s :97280 used: 12.63 s :97920 used: 12.63 s :98560 used: 13.12 s :99200 used: 13.22 s :99840 used: 14.10 s :100480 used: 13.29 s :101120 used: 12.89 s :101760 used: 13.72 s :102400 used: 32.34 s :103040 used: 18.95 s :103680 used: 18.04 s :104320 used: 16.24 s :104960 used: 16.54 s :105600 used: 15.47 s :106240 used: 14.43 s :106880 used: 12.34 s :107520 used: 12.96 s :108160 used: 13.08 s :108800 used: 12.92 s :109440 used: 12.67 s :110080 used: 13.69 s :110720 used: 13.46 s :111360 used: 12.92 s :112000 used: 12.75 s :112640 used: 16.70 s :113280 used: 32.61 s :113920 used: 17.61 s :114560 used: 18.19 s :115200 used: 16.89 s :115840 used: 18.57 s :116480 used: 15.07 s :117120 used: 12.35 s :117760 used: 31.65 s :118400 used: 17.40 s :119040 used: 18.46 s :119680 used: 17.07 s :120320 used: 17.75 s :120960 used: 13.86 s :121600 used: 13.28 s :122240 used: 12.81 s :122880 used: 13.08 s :123520 used: 14.11 s :124160 used: 13.58 s :124800 used: 13.11 s :125440 used: 13.35 s :126080 used: 13.78 s :126720 used: 14.05 s :127360 used: 13.48 s :128000 used: 12.87 s :128640 used: 13.06 s :129280 used: 12.90 s :129920 used: 13.00 s :130560 used: 14.93 s :131200 used: 13.34 s :131840 used: 12.99 s :132480 used: 12.94 s :133120 used: 13.88 s :133760 used: 13.38 s :134400 used: 12.94 s :135040 used: 13.18 s :135680 used: 13.26 s :136320 used: 12.97 s :136960 used: 13.15 s :137600 used: 13.38 s :138240 used: 13.21 s :138880 used: 13.73 s :139520 used: 19.57 s :140160 used: 22.26 s :140800 used: 16.84 s :141440 used: 12.99 s :142080 used: 13.22 s :142720 used: 13.00 s :143360 used: 13.23 s :144000 used: 13.31 s :144640 used: 13.00 s :145280 used: 13.01 s :145920 used: 12.97 s :146560 used: 13.07 s :147200 used: 12.99 s :147840 used: 13.33 s :148480 used: 19.13 s :149120 used: 28.50 s :149760 used: 20.31 s :150400 used: 20.27 s :151040 used: 13.28 s :151680 used: 11.91 s :152320 used: 13.02 s :152960 used: 13.27 s :153600 used: 14.29 s :154240 used: 13.66 s :154880 used: 12.96 s :155520 used: 13.16 s :156160 used: 13.43 s :156800 used: 13.71 s :157440 used: 13.45 s :158080 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shj16110/anaconda2/envs/3.6.1/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: generator 'test_data_generator' raised StopIteration\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 6s, sys: 20.4 s, total: 5min 26s\n",
      "Wall time: 59min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = time()\n",
    "batch = 64\n",
    "\n",
    "#exit() #delete this\n",
    "#del x_train, y_train\n",
    "gc.collect()\n",
    "\n",
    "index = []\n",
    "results = []\n",
    "N=0\n",
    "for fnames, imgs in test_data_generator(batch=batch):\n",
    "    N+=1\n",
    "    if N%10==0:\n",
    "        print ('used: {:.2f} s'.format(time()-start), end=' :{} '.format(N*batch))\n",
    "        start = time()\n",
    "    predicts = model.predict(imgs)\n",
    "    predicts = np.argmax(predicts, axis=1)\n",
    "    predicts = [label_index[p] for p in predicts]\n",
    "    index.extend(fnames)\n",
    "    results.extend(predicts)\n",
    "\n",
    "df = pd.DataFrame(columns=['fname', 'label'])\n",
    "df['fname'] = index\n",
    "df['label'] = results\n",
    "\n",
    "\n",
    "df['fname'] = df['fname'].apply(lambda x:x.split('audio/')[-1])\n",
    "# df.to_csv(os.path.join(out_path, Mname.split('/')[-1]+'_submission.csv'), index=False)\n",
    "\n",
    "legal_label = np.array(['down', 'go', 'left', 'no', 'off', 'on', 'right',\n",
    "                        'silence', 'stop', 'up', 'yes']).astype('object')\n",
    "df['label'].apply(lambda x: 'unknown' if x not in legal_label else x)\n",
    "df.to_csv(os.path.join(out_path, Mname.split('/')[-1]+'_submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_label = np.array(['down', 'go', 'left', 'no', 'off', 'on', 'right',\n",
    "                        'silence', 'stop', 'up', 'yes']).astype('object')\n",
    "df['label'] = df['label'].apply(lambda x: 'unknow' if x not in legal_label else x)\n",
    "df.to_csv(os.path.join(out_path, Mname.split('/')[-1]+'_submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknow     81834\n",
       "right      14209\n",
       "off         8725\n",
       "no          7283\n",
       "on          7265\n",
       "up          5950\n",
       "silence     5779\n",
       "yes         5698\n",
       "left        5632\n",
       "go          5581\n",
       "stop        5350\n",
       "down        5232\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/F36W-31L-17-0.94048_.hdf5'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU3",
   "language": "python",
   "name": "gpu3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
