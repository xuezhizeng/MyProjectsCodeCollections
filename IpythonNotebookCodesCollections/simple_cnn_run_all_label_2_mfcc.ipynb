{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 198 ms, sys: 79 ms, total: 277 ms\n",
      "Wall time: 3.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "\n",
    "# from keras import backend as K\n",
    "# K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=24, inter_op_parallelism_threads=24)))\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import IPython.display as ipd\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "from time import time, ctime\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "from time import time, ctime\n",
    "import librosa\n",
    "\n",
    "L = 16000\n",
    "legal_labels = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "\n",
    "#src folders\n",
    "root_path = r'.'\n",
    "out_path = r'.'\n",
    "model_path = r'.'\n",
    "train_data_path = os.path.join(root_path, 'data', 'train', 'audio')\n",
    "test_data_path = os.path.join(root_path, 'data', 'test', 'audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# losses.binary_crossentropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(samples):\n",
    "    if len(samples) >= L: return samples\n",
    "    else: return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0, 0))\n",
    "\n",
    "def chop_audio(samples, L=16000, num=20):\n",
    "    for i in range(num):\n",
    "        beg = np.random.randint(0, len(samples) - L)\n",
    "        yield samples[beg: beg + L]\n",
    "\n",
    "def label_transform(labels):\n",
    "    nlabels = []\n",
    "    for label in labels:\n",
    "        if label == '_background_noise_':\n",
    "            nlabels.append('silence')\n",
    "#         elif label not in legal_labels:\n",
    "#             nlabels.append('unknown')\n",
    "        else:\n",
    "            nlabels.append(label)\n",
    "    return pd.get_dummies(pd.Series(nlabels))\n",
    "\n",
    "def custom_fft(y, fs):\n",
    "    T = 1.0 / fs\n",
    "    N = y.shape[0]\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    # FFT is simmetrical, so we take just the first half\n",
    "    # FFT is also complex, to we take just the real part (abs)\n",
    "    vals = 2.0/N * np.abs(yf[0:N//2])\n",
    "    return xf, vals\n",
    "\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "def list_wavs_fname(dirpath, ext='wav'):\n",
    "    print(dirpath)\n",
    "    fpaths = glob(os.path.join(dirpath, r'*/*' + ext))\n",
    "    pat = r'.+/(\\w+)/\\w+\\.' + ext + '$'\n",
    "    labels = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            labels.append(r.group(1))\n",
    "    pat = r'.+/(\\w+\\.' + ext + ')$'\n",
    "    fnames = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            fnames.append(r.group(1))\n",
    "    return labels, fnames\n",
    "\n",
    "\n",
    "# # From this tutorial\n",
    "# # https://github.com/librosa/librosa/blob/master/examples/LibROSA%20demo.ipynb\n",
    "# S = librosa.feature.melspectrogram(samples, sr=sample_rate, n_mels=128)\n",
    "\n",
    "# # Convert to log scale (dB). We'll use the peak power (max) as reference.\n",
    "# log_S = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "# mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=13)\n",
    "\n",
    "# # Let's pad on the first and second deltas while we're at it\n",
    "# delta2_mfcc = librosa.feature.delta(mfcc, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train/audio\n",
      "1/64 2/64 3/64 4/64 5/64 6/64 7/64 8/64 9/64 10/64 11/64 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shj16110/anaconda2/envs/3.6.1/lib/python3.6/site-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/64 13/64 14/64 15/64 16/64 17/64 18/64 19/64 20/64 21/64 22/64 23/64 24/64 25/64 26/64 27/64 28/64 29/64 30/64 31/64 32/64 33/64 34/64 35/64 36/64 37/64 38/64 39/64 40/64 41/64 42/64 43/64 44/64 45/64 46/64 47/64 48/64 49/64 50/64 51/64 52/64 53/64 54/64 55/64 56/64 57/64 58/64 59/64 60/64 61/64 62/64 63/64 64/64 (64841, 99, 161, 1) (64841, 31) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "CPU times: user 1min 49s, sys: 7.31 s, total: 1min 56s\n",
      "Wall time: 17min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels, fnames = list_wavs_fname(train_data_path)\n",
    "\n",
    "new_sample_rate = 16000\n",
    "y_train = []\n",
    "x_train = []\n",
    "\n",
    "n=0\n",
    "for label, fname in zip(labels, fnames):\n",
    "    n+=1\n",
    "    if n%1000==0:\n",
    "        print(int(n/1000), end='/{} '.format(int(len(labels)/1000)))\n",
    "    sample_rate, samples = wavfile.read(os.path.join(train_data_path, label, fname))\n",
    "    \n",
    "    samples = pad_audio(samples)\n",
    "    \n",
    "    if len(samples) > 16000:\n",
    "        n_samples = chop_audio(samples)\n",
    "    else: n_samples = [samples]\n",
    "        \n",
    "    for samples in n_samples:\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / sample_rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        \n",
    "        y_train.append(label)\n",
    "        x_train.append(specgram)\n",
    "        \n",
    "x_train = np.array(x_train)\n",
    "x_train = x_train.reshape(tuple(list(x_train.shape) + [1]))\n",
    "y_train = label_transform(y_train)\n",
    "\n",
    "label_index = y_train.columns.values\n",
    "\n",
    "y_train = y_train.values\n",
    "y_train = np.array(y_train)\n",
    "del labels, fnames\n",
    "gc.collect()\n",
    "\n",
    "X = x_train.copy()\n",
    "Y = y_train.copy()\n",
    "print (X.shape, Y.shape, type(X), type(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_index = np.array(list(['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go',\n",
    "       'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on',\n",
    "       'one', 'right', 'seven', 'sheila', 'silence', 'six', 'stop',\n",
    "       'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero'])).astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 877 ms, total: 877 ms\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# np.save('X_31label_16k.npy', X)\n",
    "# np.save('Y_31label_16k.npy', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mid start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 1.39 s, total: 1.39 s\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = np.load('X_31label_16k.npy')\n",
    "Y = np.load('Y_31label_16k.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 99, 161, 1)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 99, 161, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 98, 160, 16)       80        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 97, 159, 16)       1040      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 48, 79, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 48, 79, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 46, 77, 21)        3045      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 44, 75, 21)        3990      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 22, 37, 21)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 22, 37, 21)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 20, 35, 36)        6840      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 11, 36)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 6, 11, 36)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2376)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               304256    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 31)                7967      \n",
      "=================================================================\n",
      "Total params: 361,782\n",
      "Trainable params: 361,012\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (99, 161, 1)\n",
    "nclass = 31\n",
    "inp = Input(shape=input_shape)\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "img_1 = Convolution2D(16, kernel_size=2, activation=activations.relu)(norm_inp)\n",
    "img_1 = Convolution2D(16, kernel_size=2, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(21, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = Convolution2D(21, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(36, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(3, 3))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Flatten()(img_1)\n",
    "\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(img_1))\n",
    "dense_1 = BatchNormalization()(Dense(256, activation=activations.relu)(dense_1))\n",
    "dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "model = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/check-point-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# checkpoint\n",
    "filepath=\"models/F36W-31L-{epoch:02d}-{val_acc:.5f}_.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58356, 99, 161, 1) <class 'numpy.ndarray'>\n",
      "(58356, 31) <class 'numpy.ndarray'>\n",
      "(6485, 99, 161, 1) <class 'numpy.ndarray'>\n",
      "(6485, 31) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = lambda *x: [print(i.shape, type(i)) for i in x]\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.1, random_state=2018)\n",
    "S(x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     31.000000\n",
       "mean     209.193548\n",
       "std       47.781739\n",
       "min       16.000000\n",
       "25%      183.500000\n",
       "50%      225.000000\n",
       "75%      240.500000\n",
       "max      261.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_valid).sum().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58356 samples, validate on 6485 samples\n",
      "Epoch 1/100\n",
      "58336/58356 [============================>.] - ETA: 0s - loss: 1.6976 - acc: 0.5029Epoch 00000: val_acc improved from -inf to 0.79630, saving model to models/F36W-31L-00-0.79630_.hdf5\n",
      "58356/58356 [==============================] - 132s - loss: 1.6976 - acc: 0.5029 - val_loss: 0.7028 - val_acc: 0.7963\n",
      "Epoch 2/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.7824 - acc: 0.7639Epoch 00001: val_acc improved from 0.79630 to 0.86507, saving model to models/F36W-31L-01-0.86507_.hdf5\n",
      "58356/58356 [==============================] - 131s - loss: 0.7824 - acc: 0.7639 - val_loss: 0.4568 - val_acc: 0.8651\n",
      "Epoch 3/100\n",
      "58336/58356 [============================>.] - ETA: 0s - loss: 0.6202 - acc: 0.8130Epoch 00002: val_acc improved from 0.86507 to 0.89067, saving model to models/F36W-31L-02-0.89067_.hdf5\n",
      "58356/58356 [==============================] - 132s - loss: 0.6201 - acc: 0.8130 - val_loss: 0.3815 - val_acc: 0.8907\n",
      "Epoch 4/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.8403Epoch 00003: val_acc improved from 0.89067 to 0.89345, saving model to models/F36W-31L-03-0.89345_.hdf5\n",
      "58356/58356 [==============================] - 131s - loss: 0.5335 - acc: 0.8403 - val_loss: 0.3726 - val_acc: 0.8934\n",
      "Epoch 5/100\n",
      "58336/58356 [============================>.] - ETA: 0s - loss: 0.4753 - acc: 0.8568Epoch 00004: val_acc improved from 0.89345 to 0.89653, saving model to models/F36W-31L-04-0.89653_.hdf5\n",
      "58356/58356 [==============================] - 133s - loss: 0.4752 - acc: 0.8568 - val_loss: 0.3619 - val_acc: 0.8965\n",
      "Epoch 6/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.4428 - acc: 0.8685Epoch 00005: val_acc improved from 0.89653 to 0.90763, saving model to models/F36W-31L-05-0.90763_.hdf5\n",
      "58356/58356 [==============================] - 131s - loss: 0.4428 - acc: 0.8685 - val_loss: 0.3199 - val_acc: 0.9076\n",
      "Epoch 7/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8767Epoch 00006: val_acc improved from 0.90763 to 0.91781, saving model to models/F36W-31L-06-0.91781_.hdf5\n",
      "58356/58356 [==============================] - 131s - loss: 0.4096 - acc: 0.8767 - val_loss: 0.2934 - val_acc: 0.9178\n",
      "Epoch 8/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8866Epoch 00007: val_acc did not improve\n",
      "58356/58356 [==============================] - 133s - loss: 0.3800 - acc: 0.8865 - val_loss: 0.3042 - val_acc: 0.9110\n",
      "Epoch 9/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.8897Epoch 00008: val_acc improved from 0.91781 to 0.92290, saving model to models/F36W-31L-08-0.92290_.hdf5\n",
      "58356/58356 [==============================] - 131s - loss: 0.3653 - acc: 0.8897 - val_loss: 0.2713 - val_acc: 0.9229\n",
      "Epoch 10/100\n",
      "58336/58356 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8937Epoch 00009: val_acc did not improve\n",
      "58356/58356 [==============================] - 133s - loss: 0.3518 - acc: 0.8937 - val_loss: 0.2711 - val_acc: 0.9224\n",
      "Epoch 11/100\n",
      "58352/58356 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.8992Epoch 00010: val_acc did not improve\n",
      "58356/58356 [==============================] - 131s - loss: 0.3315 - acc: 0.8992 - val_loss: 0.2633 - val_acc: 0.9218\n",
      "Epoch 12/100\n",
      "57904/58356 [============================>.] - ETA: 1s - loss: 0.3199 - acc: 0.9042"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, batch_size=16, validation_data=(x_valid, y_valid), \n",
    "          epochs=100, shuffle=True, verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(y_valid).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save(os.path.join(model_path, 'cnn.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F58W-31L-00-0.98303_.hdf5\r\n",
      "F58W-31L-01-0.99105_.hdf5\r\n",
      "F58W-31L-02-0.99198_.hdf5\r\n",
      "F58W-31L-03-0.99355_.hdf5\r\n",
      "F58W-31L-04-0.99424_.hdf5\r\n",
      "F58W-31L-05-0.99467_.hdf5\r\n",
      "F58W-31L-06-0.99543_.hdf5\r\n",
      "F58W-31L-09-0.99549_.hdf5\r\n",
      "F58W-31L-10-0.99572_.hdf5\r\n",
      "F58W-31L-12-0.99592_.hdf5\r\n",
      "F58W-31L-16-0.99608_.hdf5\r\n",
      "F58W-31L-19-0.99612_.hdf5\r\n",
      "F58W-31L-20-0.99623_.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "!ls models|grep 36W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Mname = 'models/F58W-31L-20-0.99623_.hdf5'\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model(Mname)\n",
    "\n",
    "legal_label = np.array(['down', 'go', 'left', 'no', 'off', 'on', 'right',\n",
    "                        'silence', 'stop', 'unknown', 'up', 'yes']).astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_data_generator(batch=16):\n",
    "    fpaths = glob(os.path.join(test_data_path, '*wav'))\n",
    "    i = 0\n",
    "    for path in fpaths:\n",
    "        if i == 0:\n",
    "            imgs = []\n",
    "            fnames = []\n",
    "        i += 1\n",
    "        rate, samples = wavfile.read(path)\n",
    "        samples = pad_audio(samples)\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        imgs.append(specgram)\n",
    "        fnames.append(path.split('\\\\')[-1])\n",
    "        if i == batch:\n",
    "            i = 0\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "            yield fnames, imgs\n",
    "    if i < batch:\n",
    "        imgs = np.array(imgs)\n",
    "        imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "        yield fnames, imgs\n",
    "    raise StopIteration()\n",
    "    \n",
    "new_sample_rate=16000\n",
    "\n",
    "label_index = np.array(list(['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go',\n",
    "       'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on',\n",
    "       'one', 'right', 'seven', 'sheila', 'silence', 'six', 'stop',\n",
    "       'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero'])).astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 12.21 s :640 used: 12.69 s :1280 used: 13.40 s :1920 used: 13.18 s :2560 used: 12.93 s :3200 used: 12.91 s :3840 used: 12.70 s :4480 used: 13.26 s :5120 used: 12.98 s :5760 used: 13.45 s :6400 used: 13.06 s :7040 used: 12.78 s :7680 used: 13.02 s :8320 used: 12.79 s :8960 used: 13.00 s :9600 used: 13.08 s :10240 used: 13.72 s :10880 used: 26.18 s :11520 used: 27.21 s :12160 used: 20.35 s :12800 used: 18.78 s :13440 used: 16.78 s :14080 used: 12.37 s :14720 used: 12.64 s :15360 used: 12.69 s :16000 used: 12.89 s :16640 used: 13.67 s :17280 used: 12.91 s :17920 used: 27.25 s :18560 used: 25.43 s :19200 used: 20.79 s :19840 used: 18.83 s :20480 used: 15.52 s :21120 used: 12.43 s :21760 used: 12.05 s :22400 used: 12.61 s :23040 used: 12.18 s :23680 used: 12.30 s :24320 used: 12.41 s :24960 used: 11.84 s :25600 used: 11.82 s :26240 used: 12.59 s :26880 used: 12.08 s :27520 used: 12.43 s :28160 used: 12.12 s :28800 used: 13.11 s :29440 used: 13.15 s :30080 used: 12.71 s :30720 used: 12.86 s :31360 used: 13.03 s :32000 used: 13.53 s :32640 used: 12.80 s :33280 used: 13.34 s :33920 used: 13.14 s :34560 used: 12.95 s :35200 used: 13.08 s :35840 used: 13.43 s :36480 used: 12.96 s :37120 used: 12.92 s :37760 used: 13.29 s :38400 used: 13.63 s :39040 used: 13.43 s :39680 used: 35.03 s :40320 used: 22.06 s :40960 used: 20.97 s :41600 used: 19.69 s :42240 used: 13.62 s :42880 used: 12.35 s :43520 used: 12.91 s :44160 used: 14.01 s :44800 used: 13.15 s :45440 used: 12.96 s :46080 used: 13.32 s :46720 used: 18.09 s :47360 used: 32.56 s :48000 used: 19.83 s :48640 used: 19.34 s :49280 used: 18.93 s :49920 used: 12.38 s :50560 used: 12.55 s :51200 used: 12.78 s :51840 used: 13.72 s :52480 used: 12.99 s :53120 used: 13.23 s :53760 used: 13.07 s :54400 used: 12.82 s :55040 used: 13.04 s :55680 used: 12.89 s :56320 used: 13.07 s :56960 used: 13.21 s :57600 used: 12.94 s :58240 used: 13.14 s :58880 used: 13.18 s :59520 used: 13.37 s :60160 used: 13.03 s :60800 used: 12.99 s :61440 used: 13.06 s :62080 used: 22.70 s :62720 used: 27.02 s :63360 used: 21.79 s :64000 used: 20.91 s :64640 used: 14.20 s :65280 used: 12.52 s :65920 used: 12.77 s :66560 used: 14.00 s :67200 used: 12.96 s :67840 used: 13.26 s :68480 used: 12.87 s :69120 used: 13.80 s :69760 used: 13.09 s :70400 used: 13.18 s :71040 used: 13.73 s :71680 used: 12.88 s :72320 used: 12.81 s :72960 used: 13.06 s :73600 used: 12.87 s :74240 used: 12.93 s :74880 used: 12.93 s :75520 used: 13.30 s :76160 used: 12.94 s :76800 used: 13.56 s :77440 used: 12.62 s :78080 used: 12.92 s :78720 used: 13.76 s :79360 used: 12.98 s :80000 used: 13.82 s :80640 used: 13.14 s :81280 used: 12.99 s :81920 used: 12.77 s :82560 used: 12.56 s :83200 used: 13.75 s :83840 used: 12.94 s :84480 used: 12.61 s :85120 used: 13.05 s :85760 used: 12.97 s :86400 used: 12.94 s :87040 used: 13.59 s :87680 used: 13.42 s :88320 used: 13.07 s :88960 used: 13.18 s :89600 used: 13.48 s :90240 used: 13.14 s :90880 used: 12.91 s :91520 used: 28.45 s :92160 used: 21.03 s :92800 used: 20.20 s :93440 used: 18.70 s :94080 used: 15.91 s :94720 used: 12.98 s :95360 used: 13.64 s :96000 used: 13.23 s :96640 used: 13.28 s :97280 used: 13.78 s :97920 used: 13.17 s :98560 used: 13.27 s :99200 used: 13.14 s :99840 used: 12.97 s :100480 used: 13.23 s :101120 used: 13.65 s :101760 used: 12.85 s :102400 used: 13.73 s :103040 used: 13.06 s :103680 used: 13.69 s :104320 used: 13.61 s :104960 used: 13.52 s :105600 used: 13.13 s :106240 used: 12.68 s :106880 used: 12.82 s :107520 used: 12.64 s :108160 used: 11.88 s :108800 used: 12.08 s :109440 used: 11.95 s :110080 used: 12.14 s :110720 used: 12.71 s :111360 used: 12.08 s :112000 used: 12.02 s :112640 used: 11.80 s :113280 used: 12.34 s :113920 used: 11.82 s :114560 used: 13.46 s :115200 used: 13.06 s :115840 "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = time()\n",
    "\n",
    "\n",
    "#exit() #delete this\n",
    "#del x_train, y_train\n",
    "gc.collect()\n",
    "\n",
    "index = []\n",
    "results = []\n",
    "N=0\n",
    "for fnames, imgs in test_data_generator(batch=batch):\n",
    "    N+=1\n",
    "    if N%10==0:\n",
    "        print ('used: {:.2f} s'.format(time()-start), end=' :{} '.format(N*batch))\n",
    "        start = time()\n",
    "    predicts = model.predict(imgs)\n",
    "    predicts = np.argmax(predicts, axis=1)\n",
    "    predicts = [label_index[p] for p in predicts]\n",
    "    index.extend(fnames)\n",
    "    results.extend(predicts)\n",
    "\n",
    "df = pd.DataFrame(columns=['fname', 'label'])\n",
    "df['fname'] = index\n",
    "df['label'] = results\n",
    "\n",
    "\n",
    "df['fname'] = df['fname'].apply(lambda x:x.split('audio/')[-1])\n",
    "# df.to_csv(os.path.join(out_path, Mname.split('/')[-1]+'_submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legal_label = np.array(['down', 'go', 'left', 'no', 'off', 'on', 'right',\n",
    "                        'silence', 'stop', 'up', 'yes']).astype('object')\n",
    "df['label'].apply(lambda x: 'unknow' if x not in legal_label else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(out_path, Mname.split('/')[-1]+'_submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU3",
   "language": "python",
   "name": "gpu3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
