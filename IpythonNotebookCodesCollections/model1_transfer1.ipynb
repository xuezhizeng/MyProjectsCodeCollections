{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras/issues/1538"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gogul09.github.io/software/flower-recognition-deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/preprocessing/image/\n",
    "\n",
    "https://keras.io/applications/#mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator =  gen.flow_from_directory(\n",
    "    directory = 'train/train',\n",
    "    target_size=(224,224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    #save_to_dir='preview'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = gen.flow_from_directory(\n",
    "        directory = 'train/validation',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = gen.flow_from_directory(\n",
    "        directory = 'train/test',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_samples = 20000\n",
    "nb_validation_samples = 2500\n",
    "epochs = 50\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "top_model_weights_path = 'model/bottleneck_fc_model.h5'\n",
    "train_data_dir = 'train/train'\n",
    "validation_data_dir = 'train/validation'\n",
    "\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.MobileNet(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        shuffle=False)\n",
    "    \n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, math.ceil(nb_train_samples / batch_size),\n",
    "        verbose=1)\n",
    "    \n",
    "    np.save('model/bottleneck_features_train.npy',\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        shuffle=False)\n",
    "    \n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, math.ceil(nb_validation_samples / batch_size),\n",
    "        verbose=1)\n",
    "    \n",
    "    np.save('model/bottleneck_features_validation.npy',\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#save_bottlebeck_features()\n",
    "#train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "    train_data = np.load('model/bottleneck_features_train.npy')\n",
    "    train_labels = np.array(\n",
    "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "    validation_data = np.load('model/bottleneck_features_validation.npy')\n",
    "    validation_labels = np.array(\n",
    "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    filepath=\"bottleneck_fc_model-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "    \n",
    "    callbacks_list = [\n",
    "    ModelCheckpoint('model/'+filepath, monitor='val_acc', verbose=1, save_best_only=True),\n",
    "    EarlyStopping(monitor='val_acc', patience=10, verbose=0)\n",
    "]\n",
    "\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels),\n",
    "              callbacks=callbacks_list)\n",
    "    model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_bottlebeck_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 24s 1ms/step - loss: 0.6169 - acc: 0.9581 - val_loss: 0.2685 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98120, saving model to model/bottleneck_fc_model-01-0.9812.hdf5\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.4166 - acc: 0.9721 - val_loss: 0.2746 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.3400 - acc: 0.9772 - val_loss: 0.2500 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98120 to 0.98360, saving model to model/bottleneck_fc_model-03-0.9836.hdf5\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.2961 - acc: 0.9800 - val_loss: 0.3101 - val_acc: 0.9796\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.2358 - acc: 0.9846 - val_loss: 0.2723 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.2384 - acc: 0.9844 - val_loss: 0.2527 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.2264 - acc: 0.9852 - val_loss: 0.2379 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.98360 to 0.98400, saving model to model/bottleneck_fc_model-07-0.9840.hdf5\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.2223 - acc: 0.9855 - val_loss: 0.2471 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1940 - acc: 0.9871 - val_loss: 0.3140 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1999 - acc: 0.9870 - val_loss: 0.2234 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.98400 to 0.98480, saving model to model/bottleneck_fc_model-10-0.9848.hdf5\n",
      "Epoch 11/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1772 - acc: 0.9885 - val_loss: 0.2286 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1938 - acc: 0.9872 - val_loss: 0.2653 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 13/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1799 - acc: 0.9883 - val_loss: 0.2924 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 14/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1682 - acc: 0.9891 - val_loss: 0.2297 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1902 - acc: 0.9877 - val_loss: 0.2299 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.98480 to 0.98520, saving model to model/bottleneck_fc_model-15-0.9852.hdf5\n",
      "Epoch 16/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1533 - acc: 0.9900 - val_loss: 0.2853 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 17/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1466 - acc: 0.9903 - val_loss: 0.2295 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 18/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1479 - acc: 0.9903 - val_loss: 0.2456 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 19/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1422 - acc: 0.9908 - val_loss: 0.2472 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 20/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1507 - acc: 0.9904 - val_loss: 0.2204 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.98520 to 0.98560, saving model to model/bottleneck_fc_model-20-0.9856.hdf5\n",
      "Epoch 21/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1403 - acc: 0.9910 - val_loss: 0.3229 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 22/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1289 - acc: 0.9916 - val_loss: 0.2330 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 23/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1333 - acc: 0.9912 - val_loss: 0.2799 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 24/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1179 - acc: 0.9923 - val_loss: 0.2170 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.98560 to 0.98600, saving model to model/bottleneck_fc_model-24-0.9860.hdf5\n",
      "Epoch 25/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1078 - acc: 0.9929 - val_loss: 0.2288 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 26/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1139 - acc: 0.9925 - val_loss: 0.2114 - val_acc: 0.9864\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.98600 to 0.98640, saving model to model/bottleneck_fc_model-26-0.9864.hdf5\n",
      "Epoch 27/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1093 - acc: 0.9929 - val_loss: 0.2169 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 28/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1091 - acc: 0.9929 - val_loss: 0.2268 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 29/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.0997 - acc: 0.9935 - val_loss: 0.2396 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 30/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1133 - acc: 0.9925 - val_loss: 0.2571 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00030: val_acc did not improve\n",
      "Epoch 31/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1148 - acc: 0.9924 - val_loss: 0.2017 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.98640 to 0.98720, saving model to model/bottleneck_fc_model-31-0.9872.hdf5\n",
      "Epoch 32/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.0930 - acc: 0.9940 - val_loss: 0.2154 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 33/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1020 - acc: 0.9933 - val_loss: 0.2448 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00033: val_acc did not improve\n",
      "Epoch 34/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.0912 - acc: 0.9940 - val_loss: 0.2253 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00034: val_acc did not improve\n",
      "Epoch 35/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.0859 - acc: 0.9945 - val_loss: 0.2254 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00035: val_acc did not improve\n",
      "Epoch 36/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.0914 - acc: 0.9940 - val_loss: 0.2267 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00036: val_acc did not improve\n",
      "Epoch 37/50\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1275 - acc: 0.9918 - val_loss: 0.2422 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00037: val_acc did not improve\n",
      "Epoch 38/50\n",
      "20000/20000 [==============================] - 21s 1ms/step - loss: 0.0841 - acc: 0.9946 - val_loss: 0.2378 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00038: val_acc did not improve\n",
      "Epoch 39/50\n",
      "20000/20000 [==============================] - 21s 1ms/step - loss: 0.0754 - acc: 0.9952 - val_loss: 0.2181 - val_acc: 0.9864\n",
      "\n",
      "Epoch 00039: val_acc did not improve\n",
      "Epoch 40/50\n",
      "20000/20000 [==============================] - 21s 1ms/step - loss: 0.0760 - acc: 0.9951 - val_loss: 0.2224 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00040: val_acc did not improve\n",
      "Epoch 41/50\n",
      "20000/20000 [==============================] - 21s 1ms/step - loss: 0.0768 - acc: 0.9950 - val_loss: 0.2296 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00041: val_acc did not improve\n"
     ]
    }
   ],
   "source": [
    "train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# # build the VGG16 network\n",
    "# model = applications.MobileNet(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# generator = datagen.flow_from_directory(\n",
    "#     train_data_dir,\n",
    "#     target_size=(img_width, img_height),\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='binary',\n",
    "#     shuffle=False)\n",
    "\n",
    "# bottleneck_features_train = model.predict_generator(\n",
    "#     generator, nb_train_samples // batch_size,\n",
    "#     verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 7, 7, 1024)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.save('model/bottleneck_features_train.npy',bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 1024)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_features_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "vd = np.load('model/bottleneck_features_validation.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2496, 7, 7, 1024)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 2 classes.\n",
      "166/166 [==============================] - 8s 48ms/step\n"
     ]
    }
   ],
   "source": [
    "generator = datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False)\n",
    "\n",
    "bottleneck_features_validation = model.predict_generator(\n",
    "    generator, nb_validation_samples // batch_size,\n",
    "    verbose=1)\n",
    "\n",
    "# np.save('model/bottleneck_features_validation.npy',\n",
    "#         bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(2500/16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_width, img_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.flow_from_directory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "    './test0',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = keras.applications.MobileNet(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 8s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "test_array = pre_model.predict_generator(generator, 1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test\\\\0.jpg',\n",
       " 'test\\\\1.jpg',\n",
       " 'test\\\\10.jpg',\n",
       " 'test\\\\100.jpg',\n",
       " 'test\\\\101.jpg',\n",
       " 'test\\\\102.jpg',\n",
       " 'test\\\\103.jpg',\n",
       " 'test\\\\104.jpg',\n",
       " 'test\\\\105.jpg',\n",
       " 'test\\\\106.jpg',\n",
       " 'test\\\\107.jpg',\n",
       " 'test\\\\108.jpg',\n",
       " 'test\\\\109.jpg',\n",
       " 'test\\\\11.jpg',\n",
       " 'test\\\\110.jpg',\n",
       " 'test\\\\111.jpg',\n",
       " 'test\\\\112.jpg',\n",
       " 'test\\\\113.jpg',\n",
       " 'test\\\\114.jpg',\n",
       " 'test\\\\115.jpg']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.filenames[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7, 7, 1024)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final = keras.models.load_model('./model/bottleneck_fc_model-31-0.9872.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 1024)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 7, 1024)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array[0].reshape(-1,7,7,1024).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.predict_on_batch(test_array[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.predict_proba(test_array[1].reshape(-1,7,7,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.predict_proba(test_array[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict helper func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(start, end):\n",
    "    result = []\n",
    "    for p in range(start,end+1):\n",
    "\n",
    "        result.append([p,final.predict_proba(\n",
    "            pre_model.predict(\n",
    "                img_to_array(load_img('./test/test/{}.jpg'.format(p), \n",
    "                                      target_size=(224,224,3)\n",
    "                                     )).reshape(-1,224,224,3)/255.0\n",
    "            ))[0][0]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def helper_batch(start, end):\n",
    "#     result = []\n",
    "#     array=[]\n",
    "#     for p in range(start,end+1):\n",
    "#         array.append(\n",
    "#         img_to_array(load_img('./test/test/{}.jpg'.format(p), \n",
    "#                                       target_size=(224,224,3)\n",
    "#                                      ))/255.0)\n",
    "            \n",
    "\n",
    "\n",
    "#         result.append(final.predict_proba(\n",
    "#             pre_model.predict(\n",
    "#                 )            \n",
    "            \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 2)\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.DataFrame(helper(1,12500))\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['id','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.predict_proba == final.predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('transfer_mobelnet3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper2(start, end):\n",
    "    result = []\n",
    "    for p in range(start,end+1):\n",
    "\n",
    "        result.append(final.predict_proba(\n",
    "            pre_model.predict(\n",
    "                img_to_array(load_img('./test1/test/{}.jpg'.format(p), \n",
    "                                      target_size=(224,224,3)\n",
    "                                     )).reshape(-1,224,224,3)/255.0\n",
    "            ))[0][0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df2 = pd.DataFrame(helper2(0,999))\n",
    "print (df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['file']=df2.index\n",
    "df2['file'] = df2['file'].apply(lambda x: str(x)+'.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns=['label','file']\n",
    "df2 = df2[['file','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['label'] = (df2['label']<0.5).map({True:'cat', False:'dog'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('transfersmall2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>970.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>971.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>972.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>973.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>974.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>975.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>976.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>977.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>978.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>979.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>980.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>981.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>982.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>983.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>984.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>985.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>986.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>987.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>988.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>989.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>990.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>991.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>992.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>993.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>994.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        file label\n",
       "0      0.jpg   cat\n",
       "1      1.jpg   cat\n",
       "2      2.jpg   cat\n",
       "3      3.jpg   cat\n",
       "4      4.jpg   dog\n",
       "5      5.jpg   dog\n",
       "6      6.jpg   dog\n",
       "7      7.jpg   dog\n",
       "8      8.jpg   cat\n",
       "9      9.jpg   cat\n",
       "10    10.jpg   dog\n",
       "11    11.jpg   cat\n",
       "12    12.jpg   dog\n",
       "13    13.jpg   cat\n",
       "14    14.jpg   dog\n",
       "15    15.jpg   dog\n",
       "16    16.jpg   dog\n",
       "17    17.jpg   cat\n",
       "18    18.jpg   dog\n",
       "19    19.jpg   dog\n",
       "20    20.jpg   dog\n",
       "21    21.jpg   dog\n",
       "22    22.jpg   dog\n",
       "23    23.jpg   dog\n",
       "24    24.jpg   cat\n",
       "25    25.jpg   dog\n",
       "26    26.jpg   cat\n",
       "27    27.jpg   dog\n",
       "28    28.jpg   dog\n",
       "29    29.jpg   dog\n",
       "..       ...   ...\n",
       "970  970.jpg   cat\n",
       "971  971.jpg   cat\n",
       "972  972.jpg   dog\n",
       "973  973.jpg   dog\n",
       "974  974.jpg   cat\n",
       "975  975.jpg   cat\n",
       "976  976.jpg   cat\n",
       "977  977.jpg   dog\n",
       "978  978.jpg   dog\n",
       "979  979.jpg   dog\n",
       "980  980.jpg   dog\n",
       "981  981.jpg   dog\n",
       "982  982.jpg   dog\n",
       "983  983.jpg   cat\n",
       "984  984.jpg   cat\n",
       "985  985.jpg   cat\n",
       "986  986.jpg   cat\n",
       "987  987.jpg   dog\n",
       "988  988.jpg   cat\n",
       "989  989.jpg   cat\n",
       "990  990.jpg   cat\n",
       "991  991.jpg   cat\n",
       "992  992.jpg   dog\n",
       "993  993.jpg   dog\n",
       "994  994.jpg   dog\n",
       "995  995.jpg   dog\n",
       "996  996.jpg   dog\n",
       "997  997.jpg   cat\n",
       "998  998.jpg   cat\n",
       "999  999.jpg   dog\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
