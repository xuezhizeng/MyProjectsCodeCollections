{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "os.chdir('D:/Dropbox/FreelancerDataMining/names')\n",
    "\n",
    "# *** To do 1: tune model parameters (if any) and add additional models ***\n",
    "method = ['SVM','NB','ME'][0]\n",
    "# Target variable: use fname.gender for now, namely gender from first name\n",
    "# fname.gender will be updated once first names for some users become available on website\n",
    "\n",
    "truth = ['fname.gender','pic.gender'][0]\n",
    "# Whether or not to over sample female to get balanced training set\n",
    "# Use original imbalanced for now\n",
    "balanced_sample = False\n",
    "\n",
    "# bi-gram of username\n",
    "def extract_features2(name, N=2):\n",
    "    name = name.lower()\n",
    "    features = {'nchar': len(name)}\n",
    "    for i in range(len(name)-N):\n",
    "        features[\"count({})\".format(name[i:i+N])] = name.lower().count(name[i:i+N])\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "    return features\n",
    "\n",
    "# features extracted from username\n",
    "def extract_features(name):\n",
    "    name = name.lower()\n",
    "    features = {\n",
    "        'last': name[-1],\n",
    "        'last_two': name[-2:],\n",
    "        'last_three': name[-3:],\n",
    "        'first': name[0],\n",
    "        'first2': name[:1],\n",
    "        'first3': name[:2],\n",
    "        'nchar': len(name),\n",
    "        'vowels.pct': sum(c in 'aoeiu' for c in name)/len(name),\n",
    "        'digits.pct': sum(c.isdigit() for c in name)/len(name),\n",
    "        'endwd': name[-1].isdigit(),\n",
    "    }\n",
    "    # commented because these features are not useful\n",
    "    #for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        #features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "    return features\n",
    "\n",
    "# *** To do 2: Extend the extract_features function to incorporate additional info such as users' profile info\n",
    "# Note that extra features does not necessarily make the model better off\n",
    "\n",
    "\n",
    "# 1. Load users\n",
    "users = pd.read_csv('users.gender.golden.csv', na_values='')\n",
    "# Focus on users with usernames\n",
    "users = users[users['username'].notnull()]\n",
    "# Only use users with predicted gender (by firstname) to train a model\n",
    "if truth=='fname.gender':\n",
    "    cand = users[users[truth].notnull() & abs(users['male.prob']-0.5)>0.4]\n",
    "if truth=='pic.gender': \n",
    "    cand = users[users[truth].notnull()]\n",
    "if balanced_sample:\n",
    "    cand_pos = cand[cand[truth]=='male']\n",
    "    cand_neg = cand[cand[truth]!='male']\n",
    "    train = pd.concat([cand, cand_neg.sample(len(cand_pos)-len(cand_neg), replace=True)])\n",
    "else: train = cand\n",
    "print('Number of obs in training set:', len(train))\n",
    "Xy = [(extract_features(n), g) for n, g in zip(train['username'], train[truth])]\n",
    "\n",
    "\n",
    "# *** To do 3: Some feature selection and standardization (z-score) might be helpful before cross validation***\n",
    "\n",
    "\n",
    "# 2. Cross-Valiation\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(Xy):\n",
    "    train = [Xy[i] for i in train_idx]\n",
    "    test = [Xy[i] for i in test_idx]\n",
    "    if method == 'SVM':\n",
    "        classifier = SklearnClassifier(SVC(kernel='linear', C=10, random_state=1), sparse=True).train(train)\n",
    "    if method == 'NB':\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "    if method == 'ME':\n",
    "        classifier = nltk.classify.MaxentClassifier.train(train, trace=3, max_iter=50)    \n",
    "    accu.append( nltk.classify.util.accuracy(classifier, test) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "# select the best model based on CV performance shown below\n",
    "print('Final accuracy:', np.mean(accu))    \n",
    "    \n",
    "\n",
    "# 3. Train model on the entire training set and generate predictions.\n",
    "if method == 'SVM':\n",
    "    classifier = SklearnClassifier(SVC(kernel='linear', C=10, random_state=1), sparse=True).train(Xy)\n",
    "if method == 'NB':\n",
    "    classifier = nltk.NaiveBayesClassifier.train(Xy)\n",
    "if method == 'ME':\n",
    "    classifier = nltk.classify.MaxentClassifier.train(Xy, trace=3, max_iter=50)\n",
    "users['uname.gender'] = [classifier.classify(extract_features(e)) for e in users.username]\n",
    "users.to_csv('users.%s.%s.%s.csv' % (method, truth, balanced_sample), index=False, na_rep='')\n",
    "sub = users[users[truth].notnull()]\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(sub[truth], sub['uname.gender'])/len(sub))\n",
    "print('Done!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
